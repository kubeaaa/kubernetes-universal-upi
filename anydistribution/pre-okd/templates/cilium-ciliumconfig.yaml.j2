---
# https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
apiVersion: cilium.io/v1alpha1
kind: CiliumConfig
metadata:
  name: cilium
  namespace: cilium
spec:
  # -- (string) Kubernetes service host
  k8sServiceHost: "api.{{ k8s_dns_zone[:-1] }}"
  # -- (string) Kubernetes service port
  k8sServicePort: "6443"

  # -- Enable installation of PodCIDR routes between worker
  # nodes if worker nodes share a common L2 network segment.
  autoDirectNodeRoutes: false

  # -- Configure Kubernetes specific configuration
  k8s: {}

  bpf:
    # -- Enable BPF clock source probing for more efficient tick retrieval.
    clockProbe: true

    # -- Enables pre-allocation of eBPF map values. This increases
    # memory usage but can reduce latency.
    preallocateMaps: true

    # -- (bool) Enable native IP masquerade support in eBPF
    # @default -- `false`
    masquerade: true

    # -- (bool) Configure whether direct routing mode should route traffic via
    # host stack (true) or directly and more efficiently out of BPF (false) if
    # the kernel supports it. The latter has the implication that it will also
    # bypass netfilter in the host namespace.
    # @default -- `false`
    hostLegacyRouting: true

    # -- (bool) Configure the eBPF-based TPROXY to reduce reliance on iptables rules
    # for implementing Layer 7 policy.
    # @default -- `false`
    tproxy: true

  ipv4:
    # -- Enable IPv4 support (default true).
    enabled: true

  ipv6:
    # -- Enable IPv6 support.
    enabled: false

  # -- Configure whether to install iptables rules to allow for TPROXY
  # (L7 proxy injection), iptables-based masquerading and compatibility
  # with kube-proxy.
  installIptablesRules: true

  # -- Install Iptables rules to skip netfilter connection tracking on all pod
  # traffic. This option is only effective when Cilium is running in direct
  # routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
  # is running in a managed Kubernetes environment or in a chained CNI setup.
  installNoConntrackIptablesRules: false

  ipam:
    # -- Configure IP Address Management mode.
    # ref: https://docs.cilium.io/en/stable/concepts/networking/ipam/
    mode: "cluster-pool"
    operator:
      # -- Deprecated in favor of ipam.operator.clusterPoolIPv4PodCIDRList.
      # IPv4 CIDR range to delegate to individual nodes for IPAM.
      clusterPoolIPv4PodCIDR: ""

      # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
      clusterPoolIPv4PodCIDRList:
      {% for cidr in (os_cluster_network_cidrs | map(attribute='cidr') | list) %}- {{ cidr }}
      {% endfor %}

      # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
      clusterPoolIPv4MaskSize: "{{ cni_cilium_cluster_pool_ipv4_mask_size }}"

  # -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
  # When specified, Cilium assumes networking for this CIDR is preconfigured and
  # hands traffic destined for that range to the Linux network stack without
  # applying any SNAT.
  # Generally speaking, specifying a native routing CIDR implies that Cilium can
  # depend on the underlying networking stack to route packets to their
  # destination. To offer a concrete example, if Cilium is configured to use
  # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
  # the user must configure the routes to reach pods, either manually or by
  # setting the auto-direct-node-routes flag.
  ipv4NativeRoutingCIDR: "{{ cni_cilium_ipv4_native_routing_cidr }}"

  endpointRoutes:
    # -- Enable use of per endpoint routes instead of routing via
    # the cilium_host interface.
    enabled: true

  # -- Configure the kube-proxy replacement in Cilium BPF datapath
  # Valid options are "disabled", "partial", "strict".
  # ref: https://docs.cilium.io/en/stable/gettingstarted/kubeproxy-free/
  kubeProxyReplacement: "strict"

  # -- Configure the encapsulation configuration for communication between nodes.
  # Possible values:
  #   - disabled
  #   - vxlan (default)
  #   - geneve
  tunnel: vxlan

  # -- Tunnel port (default 8472 for "vxlan" and 6081 for "geneve")
  tunnelPort: 0

  cni:
    # -- Configure the path to the CNI binary directory on the host.
    binPath: "/var/lib/cni/bin"

    # -- Configure the path to the CNI configuration directory on the host.
    confPath: "/var/run/multus/cni/net.d"

    # -- Configure chaining on top of other CNI plugins. Possible values:
    #  - none
    #  - aws-cni
    #  - flannel
    #  - generic-veth
    #  - portmap
    chainingMode: none

  # -- Configure prometheus metrics on the configured port at /metrics
  prometheus:
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: false

  hubble:
    # -- TLS configuration for Hubble
    tls:
      # -- Enable mutual TLS for listenAddress. Setting this value to false is
      # highly discouraged as the Hubble API provides access to potentially
      # sensitive network flow metadata and is exposed on the host network.
      enabled: false

  # -- Security context to be added to agent pods
  securityContext:
    # runAsUser: 0
    privileged: true
